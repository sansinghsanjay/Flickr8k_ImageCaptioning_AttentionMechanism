{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":1271,"status":"ok","timestamp":1619897762436,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"Dt3eFVmJU_iI","outputId":"133b1f94-7a6c-46cd-8232-b8852bef7d7b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nSanjay Singh\\nsan.singhsanjay@gmail.com\\nApril-2021\\nInference of Attention Mechanism for Image Captioning - Most part taken from Google Tutorials\\nImplementation of:\\nhttps://github.com/subhamio/image-captioning-using-attention-mechanism-local-attention-and-global-attention-/blob/master/image_captioning_using_attention_mechanism.ipynb\\n'"]},"execution_count":1,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["'''\n","Sanjay Singh\n","san.singhsanjay@gmail.com\n","April-2021\n","Inference of Attention Mechanism for Image Captioning - Most part taken from Google Tutorials\n","Implementation of:\n","https://github.com/subhamio/image-captioning-using-attention-mechanism-local-attention-and-global-attention-/blob/master/image_captioning_using_attention_mechanism.ipynb\n","'''"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1738,"status":"ok","timestamp":1619897762929,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"_04vbBpdVlLE","outputId":"85f5fde8-de9e-4aee-b8c7-ed69d8fdc5c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4472,"status":"ok","timestamp":1619897765685,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"Qh6BYozzVrkg"},"outputs":[],"source":["# packages\n","import tensorflow as tf\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import corpus_bleu\n","import string\n","from tqdm import tqdm\n","from numpy import array\n","from pickle import load\n","from keras.preprocessing.text import Tokenizer\n","import matplotlib.pyplot as plt\n","#from keras.backend.tensorflow_backend import set_session\n","import keras\n","import sys, time, os, warnings \n","warnings.filterwarnings(\"ignore\")\n","import re\n","import numpy as np\n","import pandas as pd \n","from PIL import Image\n","import pickle\n","from collections import Counter\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense, BatchNormalization\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","from keras.layers.merge import add\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import load_img, img_to_array\n","from sklearn.utils import shuffle\n","from keras.applications.vgg16 import VGG16, preprocess_input\n","from sklearn.model_selection import train_test_split\n","from os import listdir"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4465,"status":"ok","timestamp":1619897765693,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"iQR0jwpWVuP8"},"outputs":[],"source":["# CONSTANTS\n","EMBEDDING_DIM = 256\n","BATCH_SIZE = 131\n","BUFFER_SIZE = 1000\n","UNITS = 512"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4454,"status":"ok","timestamp":1619897765698,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"xVgjJX5ZVxIZ"},"outputs":[],"source":["# https://www.tensorflow.org/tutorials/text/image_captioning\n","class VGG16_Encoder(tf.keras.Model):\n","\t# This encoder passes the features through a Fully connected layer\n","\tdef __init__(self, EMBEDDING_DIM):\n","\t\tsuper(VGG16_Encoder, self).__init__()\n","\t\tself.fc = tf.keras.layers.Dense(EMBEDDING_DIM)\n","\t\tself.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n","\tdef call(self, x):\n","\t\tx = self.fc(x)\n","\t\tx = tf.nn.relu(x)\n","\t\treturn x"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4445,"status":"ok","timestamp":1619897765703,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"wkcfHHjPV269"},"outputs":[],"source":["class Rnn_Local_Decoder(tf.keras.Model):\n","\tdef __init__(self, embedding_dim, UNITS, vocab_size):\n","\t\tsuper(Rnn_Local_Decoder, self).__init__()\n","\t\tself.units = UNITS\n","\t\tself.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","\t\tself.gru = tf.compat.v1.keras.layers.CuDNNGRU(self.units, return_sequences=True, return_state=True,                              recurrent_initializer='glorot_uniform')\n","\t\tself.fc1 = tf.keras.layers.Dense(self.units)\n","\t\tself.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n","\t\tself.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n","\t\tself.fc2 = tf.keras.layers.Dense(vocab_size)\n","\t\t# Implementing Attention Mechanism\n","\t\tself.Uattn = tf.keras.layers.Dense(UNITS)\n","\t\tself.Wattn = tf.keras.layers.Dense(UNITS)\n","\t\tself.Vattn = tf.keras.layers.Dense(1)\n","\n","\tdef call(self, x, features, hidden):\n","\t\t# features shape ==\u003e (64,49,256) ==\u003e Output from ENCODER\n","\t\t# hidden shape == (batch_size, hidden_size) ==\u003e(64,512)\n","\t\t# hidden_with_time_axis shape == (batch_size, 1, hidden_size) ==\u003e (64,1,512)\n","\t\thidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\t\t# score shape == (64, 49, 1)\n","\t\t# Attention Function\n","\t\t'''e(ij) = f(s(t-1),h(j))'''\n","\t\t''' e(ij) = Vattn(T)*tanh(Uattn * h(j) + Wattn * s(t))'''\n","\t\tscore = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n","\t\t# self.Uattn(features) : (64,49,512)\n","\t\t# self.Wattn(hidden_with_time_axis) : (64,1,512)\n","\t\t# tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)) : (64,49,512)\n","\t\t# self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis))) : (64,49,1) ==\u003e score\n","\t\t# you get 1 at the last axis because you are applying score to self.Vattn\n","\t\t# Then find Probability using Softmax\n","\t\t'''attention_weights(alpha(ij)) = softmax(e(ij))'''\n","\t\tattention_weights = tf.nn.softmax(score, axis=1)\n","\t\t# attention_weights shape == (64, 49, 1)\n","\t\t# Give weights to the different pixels in the image\n","\t\t''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n","\t\tcontext_vector = attention_weights * features\n","\t\tcontext_vector = tf.reduce_sum(context_vector, axis=1)\n","\t\t# Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n","\t\t# context_vector shape after sum == (64, 256)\n","\t\t# x shape after passing through embedding == (64, 1, 256)\n","\t\tx = self.embedding(x)\n","\t\t# x shape after concatenation == (64, 1,  512)\n","\t\tx = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\t\t# passing the concatenated vector to the GRU\n","\t\toutput, state = self.gru(x)\n","\t\t# shape == (batch_size, max_length, hidden_size)\n","\t\tx = self.fc1(output)\n","\t\t# x shape == (batch_size * max_length, hidden_size)\n","\t\tx = tf.reshape(x, (-1, x.shape[2]))\n","\t\t# Adding Dropout and BatchNorm Layers\n","\t\tx= self.dropout(x)\n","\t\tx= self.batchnormalization(x)\n","\t\t# output shape == (64 * 512)\n","\t\tx = self.fc2(x)\n","\t\t# shape : (64 * 8329(vocab))\n","\t\treturn x, state, attention_weights\n","\n","\tdef reset_state(self, batch_size):\n","\t\treturn tf.zeros((batch_size, self.units))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4435,"status":"ok","timestamp":1619897765706,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"h6GOk3zRV6fR"},"outputs":[],"source":["def loss_function(real, pred):\n","\tmask = tf.math.logical_not(tf.math.equal(real, 0))\n","\tloss_ = loss_object(real, pred)\n","\tmask = tf.cast(mask, dtype=loss_.dtype)\n","\tloss_ *= mask\n","\treturn tf.reduce_mean(loss_)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4430,"status":"ok","timestamp":1619897765711,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"j7QGIM0KZWK1"},"outputs":[],"source":["@tf.function\n","def train_step(img_tensor, target):\n","\tloss = 0\n","\t# initializing the hidden state for each batch\n","\t# because the captions are not related from image to image\n","\thidden = decoder.reset_state(batch_size=target.shape[0])\n","\tdec_input = tf.expand_dims([wordtoix['startseq']] * BATCH_SIZE, 1)\n","\twith tf.GradientTape() as tape:\n","\t\tfeatures = encoder(img_tensor)\n","\t\tfor i in range(1, target.shape[1]):\n","\t\t\t# passing the features through the decoder\n","\t\t\tpredictions, hidden, _ = decoder(dec_input, features, hidden)\n","\t\t\tloss += loss_function(target[:, i], predictions)\n","\t\t\t# using teacher forcing\n","\t\t\tdec_input = tf.expand_dims(target[:, i], 1)\n","\ttotal_loss = (loss / int(target.shape[1]))\n","\ttrainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\tgradients = tape.gradient(loss, trainable_variables)\n","\toptimizer.apply_gradients(zip(gradients, trainable_variables))\n","\treturn loss, total_loss"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4424,"status":"ok","timestamp":1619897765714,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"qALJJsCnZZCf"},"outputs":[],"source":["# function to read, resize and preprocess an image\n","def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (224, 224))\n","    img = preprocess_input(img)\n","    return img, image_path "]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4419,"status":"ok","timestamp":1619897765717,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"H81kymkUZbdN"},"outputs":[],"source":["def map_func(img_name, cap):\n","\timg_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","\treturn img_tensor, cap"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4409,"status":"ok","timestamp":1619897765722,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"7E5gJkPxYoSu"},"outputs":[],"source":["# paths\n","images_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/train_npy_files/\"\n","img_captions_csv_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/processed_data_AttentionMech/train_image_caption_processed.csv\"\n","vocabulary_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/processed_data_AttentionMech/vocabulary.txt\"\n","max_caption_len_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/processed_data_AttentionMech/max_caption_length.txt\""]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4404,"status":"ok","timestamp":1619897765725,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"QEMtGSgrZffL"},"outputs":[],"source":["# reading dataset\n","data = pd.read_csv(img_captions_csv_path)\n","img_name = list(data['image'])\n","img_caption = list(data['caption'])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5076,"status":"ok","timestamp":1619897766405,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"A9x2ly7_ZiNT"},"outputs":[],"source":["# appending image names to their paths\n","for i in range(len(img_name)):\n","\timg_name[i] = images_path + img_name[i]"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5088,"status":"ok","timestamp":1619897766431,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"w8Q6rNnJYCYi"},"outputs":[],"source":["# reading vocabulary file\n","vocabulary = list()\n","f_ptr = open(vocabulary_path, \"r\")\n","lines = f_ptr.readlines()\n","for line in lines:\n","\tvocabulary.append(line.strip())"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5073,"status":"ok","timestamp":1619897766434,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"8xmKBO2yYEf1","outputId":"220b2933-e43f-42f1-af24-8b82e2a1a7ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary Size:  1657\n"]}],"source":["# finding size of vocabulary\n","vocab_size = len(vocabulary) + 1 # added 1 for padded zero\n","print(\"Vocabulary Size: \", vocab_size)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":5070,"status":"ok","timestamp":1619897766438,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"wE7ZOoZ1ZuPD"},"outputs":[],"source":["# making word-to-index and index-to-word dictionary\n","wordtoix = dict()\n","ixtoword = dict()\n","for i in range(len(vocabulary)):\n","\twordtoix[vocabulary[i]] = (i + 1)\n","\tixtoword[(i + 1)] = vocabulary[i]"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5066,"status":"ok","timestamp":1619897766442,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"ZU9iedtWZyYD","outputId":"19e2dde4-27c6-44aa-864f-bd5d8ddb86a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Max Caption Length:  31\n"]}],"source":["# finding max caption length\n","f_ptr = open(max_caption_len_path, 'r')\n","data = f_ptr.read()\n","f_ptr.close()\n","max_caption_len = int(data.split(\":\")[1].strip())\n","print(\"Max Caption Length: \", max_caption_len)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5063,"status":"ok","timestamp":1619897766446,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"-aFNgEvQZ1dL","outputId":"8fd501c2-03a6-4d61-df1c-45e9b2b47ee3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of all_img_names:  29999\n","Length of all_captions:  29999\n"]}],"source":["# converting captions to their indices\n","all_img_names = list()\n","all_captions = list()\n","img_caption_ix = list()\n","for i in range(len(img_caption)):\n","\tcaptions = img_caption[i].split(\"#\")\n","\tfor caption in captions:\n","\t\tall_img_names.append(img_name[i])\n","\t\tall_captions.append(caption)\n","\t\twords = caption.split(\" \")\n","\t\ttemp_list = list()\n","\t\tfor word in words:\n","\t\t\tif(word in wordtoix):\n","\t\t\t\ttemp_list.append(wordtoix[word])\n","\t\timg_caption_ix.append(temp_list)\n","# printing shape of all_img_names and all_captions\n","print(\"Length of all_img_names: \", len(all_img_names))\n","print(\"Length of all_captions: \", len(all_captions))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5059,"status":"ok","timestamp":1619897766449,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"mS6hZjg7Z4kw","outputId":"49f4afd3-3b7e-4c1b-906c-6dde465cd9a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of img_caption_padded:  (29999, 31)\n"]}],"source":["# padding zeros to each caption to make it equal to max_caption_len\n","img_caption_padded = tf.keras.preprocessing.sequence.pad_sequences(img_caption_ix, max_caption_len, padding='post')\n","print(\"Shape of img_caption_padded: \", img_caption_padded.shape)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7260,"status":"ok","timestamp":1619897768657,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"PBJhigz4Z73W","outputId":"43fccf13-e43e-4cdb-ecee-01565a928ec5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n"]}],"source":["# loading vgg-16 model to extract bottleneck features\n","image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":7248,"status":"ok","timestamp":1619897768663,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"3uE5X0klYJQA"},"outputs":[],"source":["# defining optimization and loss-object\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":7249,"status":"ok","timestamp":1619897768672,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"wPZ87eoEaAgC"},"outputs":[],"source":["# making image and caption map - for training\n","dataset = tf.data.Dataset.from_tensor_slices((all_img_names, img_caption_padded))\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","#dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":7242,"status":"ok","timestamp":1619897768679,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"IAJWYfLyYLcW"},"outputs":[],"source":["# defining encoder and decoder\n","encoder = VGG16_Encoder(EMBEDDING_DIM)\n","decoder = Rnn_Local_Decoder(EMBEDDING_DIM, UNITS, vocab_size)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7241,"status":"ok","timestamp":1619897768685,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"yFyreusRaEID","outputId":"bb798099-d845-40dc-8165-c05ba4f89247"},"outputs":[{"name":"stdout","output_type":"stream","text":["num_steps:  229\n"]}],"source":["# defining num_steps \n","num_steps = len(all_img_names) // BATCH_SIZE\n","print(\"num_steps: \", num_steps)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":7234,"status":"ok","timestamp":1619897768690,"user":{"displayName":"Gpu User","photoUrl":"","userId":"05020043369902705889"},"user_tz":-330},"id":"ncEQCg9ZYPvv","outputId":"fffaa060-a6c3-40af-93fd-4d5aa858a1ba"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/output/attention_mech_models/ckpt-1'"]},"execution_count":25,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["checkpoint_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/output/attention_mech_models/\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,decoder=decoder,optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","ckpt_manager.restore_or_initialize()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"vz11zTl7aJ4X"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken for epoch-1 and batch-1: 134.79977822303772 sec, Loss: 0.4620231505363218\n","\n","Time taken for epoch-1 and batch-101: 762.141021490097 sec, Loss: 0.43179158241518084\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-26-4a8d98e0b201\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#target = tf.reshape(target, (1, target.shape[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2574\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2575\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2576\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2577\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# training related parameters and training\n","loss_plot = []\n","start_epoch = 0\n","EPOCHS = 20\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","    for (batch, (img_tensor, target)) in enumerate(dataset):\n","        #target = tf.reshape(target, (1, target.shape[0]))\n","        batch_loss, t_loss = train_step(img_tensor, target)\n","        total_loss += t_loss\n","        if batch % 100 == 0:\n","            print(\"Time taken for epoch-{} and batch-{}: {} sec, Loss: {}\\n\".format(epoch + 1, batch + 1, time.time() - start, batch_loss.numpy() / int(target.shape[1])))\n","    # storing the epoch end loss value to plot later\n","    loss_plot.append(total_loss / num_steps)\n","    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))\n","    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4LilKMwBZKee"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNeJs575ypzuo0JjICWXajc","name":"attention_mechanism_inference.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}