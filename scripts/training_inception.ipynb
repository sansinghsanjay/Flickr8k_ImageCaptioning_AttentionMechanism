{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSanjay Singh\\nsan.singhsanjay@gmail.com\\nMay-2021\\nTraining Image Captioning by using Visual Attention Mechanism\\nFollowed this:\\nhttps://www.tensorflow.org/tutorials/text/image_captioning\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sanjay Singh\n",
    "san.singhsanjay@gmail.com\n",
    "May-2021\n",
    "Training Image Captioning by using Visual Attention Mechanism\n",
    "Followed this:\n",
    "https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import tensorflow as tf\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "#import matplotlib.pyplot as plt\n",
    "#import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "#import json\n",
    "#from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "NPY_SHAPE_1 = 64\n",
    "NPY_SHAPE_2 = 2048\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "EMBEDDING_DIMS = 256\n",
    "UNITS = 512\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "images_npy_path = \"/notebooks/inception_train_npy_files/\"\n",
    "img_captions_csv_path = \"/notebooks/output/intermediate_files/train_image_caption_processed.csv\"\n",
    "vocabulary_path = \"/notebooks/output/intermediate_files/vocabulary.txt\"\n",
    "max_caption_len_path = \"/notebooks/output/intermediate_files/max_caption_length.txt\"\n",
    "checkpoint_path = \"/notebooks/output/attention_mech_models_inception/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "\tdef __init__(self, units):\n",
    "\t\tsuper(BahdanauAttention, self).__init__()\n",
    "\t\tself.W1 = tf.keras.layers.Dense(units)\n",
    "\t\tself.W2 = tf.keras.layers.Dense(units)\n",
    "\t\tself.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "\tdef call(self, features, hidden):\n",
    "\t\t# features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\t\t# hidden shape == (batch_size, hidden_size)\n",
    "\t\t# hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "\t\thidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\t\t# attention_hidden_layer shape == (batch_size, 64, units)\n",
    "\t\tattention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "\t\t# score shape == (batch_size, 64, 1)\n",
    "\t\t# This gives you an unnormalized score for each image feature.\n",
    "\t\tscore = self.V(attention_hidden_layer)\n",
    "\t\t# attention_weights shape == (batch_size, 64, 1)\n",
    "\t\tattention_weights = tf.nn.softmax(score, axis=1)\n",
    "\t\t# context_vector shape after sum == (batch_size, hidden_size)\n",
    "\t\tcontext_vector = attention_weights * features\n",
    "\t\tcontext_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\t\treturn context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "\t# Since you have already extracted the features and dumped it\n",
    "\t# This encoder passes those features through a Fully connected layer\n",
    "\tdef __init__(self, embedding_dim):\n",
    "\t\tsuper(CNN_Encoder, self).__init__()\n",
    "\t\t# shape after fc == (batch_size, 64, embedding_dim)\n",
    "\t\tself.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.fc(x)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "\t# Since you have already extracted the features and dumped it\n",
    "\t# This encoder passes those features through a Fully connected layer\n",
    "\tdef __init__(self, embedding_dim):\n",
    "\t\tsuper(CNN_Encoder, self).__init__()\n",
    "\t\t# shape after fc == (batch_size, 64, embedding_dim)\n",
    "\t\tself.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.fc(x)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "\tdef __init__(self, embedding_dim, units, vocab_size):\n",
    "\t\tsuper(RNN_Decoder, self).__init__()\n",
    "\t\tself.units = units\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True,              recurrent_initializer='glorot_uniform')\n",
    "\t\tself.fc1 = tf.keras.layers.Dense(self.units)\n",
    "\t\tself.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\t\tself.attention = BahdanauAttention(self.units)\n",
    "\n",
    "\tdef call(self, x, features, hidden):\n",
    "\t\t# defining attention as a separate model\n",
    "\t\tcontext_vector, attention_weights = self.attention(features, hidden)\n",
    "\t\t# x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "\t\tx = self.embedding(x)\n",
    "\t\t# x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\t\tx = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\t\t# passing the concatenated vector to the GRU\n",
    "\t\toutput, state = self.gru(x)\n",
    "\t\t# shape == (batch_size, max_length, hidden_size)\n",
    "\t\tx = self.fc1(output)\n",
    "\t\t# x shape == (batch_size * max_length, hidden_size)\n",
    "\t\tx = tf.reshape(x, (-1, x.shape[2]))\n",
    "\t\t# output shape == (batch_size * max_length, vocab)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x, state, attention_weights\n",
    "\n",
    "\tdef reset_state(self, batch_size):\n",
    "\t\treturn tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "\tmask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "\tloss_ = loss_object(real, pred)\n",
    "\tmask = tf.cast(mask, dtype=loss_.dtype)\n",
    "\tloss_ *= mask\n",
    "\treturn tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load image name and captions\n",
    "def load_imagenames_captions(data_path):\n",
    "\tdata_df = pd.read_csv(data_path)\n",
    "\timagenames = list()\n",
    "\timagecaptions = list()\n",
    "\tprint(\"Loading imagenames and imagecaptions:\")\n",
    "\tfor i in tqdm(range(data_df.shape[0])):\n",
    "\t\tcaption_value = data_df.iloc[i]['caption']\n",
    "\t\tcaptions_list = caption_value.split(\"#\")\n",
    "\t\tfor caption in captions_list:\n",
    "\t\t\timagenames.append(data_df.iloc[i]['image'])\n",
    "\t\t\timagecaptions.append(caption)\n",
    "\tprint()\n",
    "\tnew_df = pd.DataFrame(columns=['image', 'caption'])\n",
    "\tnew_df['image'] = imagenames\n",
    "\tnew_df['caption'] = imagecaptions\n",
    "\t# shuffling dataframe\n",
    "\tnew_df = new_df.sample(frac=1.0).reset_index(drop=True)\n",
    "\treturn new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "\timg_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "\treturn img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target, wordtoix):\n",
    "\tloss = 0\n",
    "\t# initializing the hidden state for each batch\n",
    "\t# because the captions are not related from image to image\n",
    "\thidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\tdec_input = tf.expand_dims([wordtoix['<startseq>']] * target.shape[0], 1)\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tfeatures = encoder(img_tensor)\n",
    "\t\tfor i in range(1, target.shape[1]):\n",
    "\t\t\t# passing the features through the decoder\n",
    "\t\t\tpredictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\t\t\tloss += loss_function(target[:, i], predictions)\n",
    "\t\t\t# using teacher forcing\n",
    "\t\t\tdec_input = tf.expand_dims(target[:, i], 1)\n",
    "\ttotal_loss = (loss / int(target.shape[1]))\n",
    "\ttrainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\tgradients = tape.gradient(loss, trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\treturn loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 168/7591 [00:00<00:04, 1676.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imagenames and imagecaptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7591/7591 [00:04<00:00, 1672.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shape:  (37952, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "data_df = load_imagenames_captions(img_captions_csv_path)\n",
    "print(\"Data shape: \", data_df.shape)\n",
    "img_name_train = list(data_df['image'])\n",
    "cap_train = list(data_df['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding images_npy_path to imagenames\n",
    "for i in range(len(img_name_train)):\n",
    "\timg_name_train[i] = images_npy_path + img_name_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from vocabulary file\n",
      "Vocabulary Size (after adding 1 for padding 0):  8511\n"
     ]
    }
   ],
   "source": [
    "# reading vocabulary file\n",
    "vocabulary = list()\n",
    "f_ptr = open(vocabulary_path, \"r\")\n",
    "lines = f_ptr.readlines()\n",
    "f_ptr.close()\n",
    "for line in lines:\n",
    "\tvocabulary.append(line.strip())\n",
    "vocab_size = len(vocabulary) + 1 # 1 is added for zero\n",
    "print(\"Successfully loaded data from vocabulary file\")\n",
    "print(\"Vocabulary Size (after adding 1 for padding 0): \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created wordtoix dictionary\n"
     ]
    }
   ],
   "source": [
    "# creating wordtoix dictionary\n",
    "wordtoix = dict()\n",
    "for i in range(len(vocabulary)):\n",
    "\twordtoix[vocabulary[i]] = i\n",
    "print(\"Successfully created wordtoix dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Caption Length:  34\n"
     ]
    }
   ],
   "source": [
    "# reading maximum caption length\n",
    "f_ptr = open(max_caption_len_path, 'r')\n",
    "data = f_ptr.read()\n",
    "f_ptr.close()\n",
    "max_caption_len = int(data.split(\":\")[1].strip())\n",
    "print(\"Max Caption Length: \", max_caption_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21948/37952 [00:00<00:00, 219449.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making numerical vector for captions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 37952/37952 [00:00<00:00, 232790.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating numerical vector for captions\n",
    "cap_num_vector = list()\n",
    "print(\"Making numerical vector for captions:\")\n",
    "for caption in tqdm(cap_train):\n",
    "\twords = caption.split(' ')\n",
    "\ttemp_list = list()\n",
    "\tfor word in words:\n",
    "\t\ttemp_list.append(wordtoix[word])\n",
    "\tcap_num_vector.append(temp_list)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding zeros in numerical captions to make them all equal to max_caption_len\n",
    "cap_num_vector = tf.keras.preprocessing.sequence.pad_sequences(cap_num_vector, max_caption_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_num_vector))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making object of encoder and decoder\n",
    "encoder = CNN_Encoder(EMBEDDING_DIMS)\n",
    "decoder = RNN_Decoder(EMBEDDING_DIMS, UNITS, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating checkpoint for model\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "\tstart_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "\t# restoring the latest checkpoint in checkpoint_path\n",
    "\tckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to collect loss values\n",
    "loss_values = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.5558\n",
      "Epoch 1 Batch 100 Loss 1.6141\n",
      "Epoch 1 Batch 200 Loss 1.4223\n",
      "Epoch 1 Batch 300 Loss 1.3335\n",
      "Epoch 1 Batch 400 Loss 1.1625\n",
      "Epoch 1 Batch 500 Loss 1.3280\n",
      "Epoch 1 Loss 1.398488\n",
      "Time taken for 1 epoch 127.39 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3005\n",
      "Epoch 2 Batch 100 Loss 1.1358\n",
      "Epoch 2 Batch 200 Loss 1.1342\n",
      "Epoch 2 Batch 300 Loss 1.1066\n",
      "Epoch 2 Batch 400 Loss 1.0567\n",
      "Epoch 2 Batch 500 Loss 0.9443\n",
      "Epoch 2 Loss 1.090052\n",
      "Time taken for 1 epoch 93.71 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.0394\n",
      "Epoch 3 Batch 100 Loss 1.0428\n",
      "Epoch 3 Batch 200 Loss 0.9819\n",
      "Epoch 3 Batch 300 Loss 0.8908\n",
      "Epoch 3 Batch 400 Loss 0.9029\n",
      "Epoch 3 Batch 500 Loss 1.0373\n",
      "Epoch 3 Loss 0.977683\n",
      "Time taken for 1 epoch 93.66 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.9481\n",
      "Epoch 4 Batch 100 Loss 0.9064\n",
      "Epoch 4 Batch 200 Loss 0.8892\n",
      "Epoch 4 Batch 300 Loss 0.8920\n",
      "Epoch 4 Batch 400 Loss 0.8089\n",
      "Epoch 4 Batch 500 Loss 1.0243\n",
      "Epoch 4 Loss 0.900383\n",
      "Time taken for 1 epoch 93.70 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.8179\n",
      "Epoch 5 Batch 100 Loss 0.9129\n",
      "Epoch 5 Batch 200 Loss 0.7656\n",
      "Epoch 5 Batch 300 Loss 0.7216\n",
      "Epoch 5 Batch 400 Loss 0.9438\n",
      "Epoch 5 Batch 500 Loss 0.7901\n",
      "Epoch 5 Loss 0.836336\n",
      "Time taken for 1 epoch 93.73 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.7633\n",
      "Epoch 6 Batch 100 Loss 0.7950\n",
      "Epoch 6 Batch 200 Loss 0.8312\n",
      "Epoch 6 Batch 300 Loss 0.7250\n",
      "Epoch 6 Batch 400 Loss 0.8540\n",
      "Epoch 6 Batch 500 Loss 0.8909\n",
      "Epoch 6 Loss 0.783410\n",
      "Time taken for 1 epoch 93.77 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.7805\n",
      "Epoch 7 Batch 100 Loss 0.7947\n",
      "Epoch 7 Batch 200 Loss 0.6758\n",
      "Epoch 7 Batch 300 Loss 0.7047\n",
      "Epoch 7 Batch 400 Loss 0.7230\n",
      "Epoch 7 Batch 500 Loss 0.7481\n",
      "Epoch 7 Loss 0.736749\n",
      "Time taken for 1 epoch 93.80 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.6789\n",
      "Epoch 8 Batch 100 Loss 0.7208\n",
      "Epoch 8 Batch 200 Loss 0.6776\n",
      "Epoch 8 Batch 300 Loss 0.7214\n",
      "Epoch 8 Batch 400 Loss 0.6606\n",
      "Epoch 8 Batch 500 Loss 0.6933\n",
      "Epoch 8 Loss 0.695436\n",
      "Time taken for 1 epoch 93.94 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7168\n",
      "Epoch 9 Batch 100 Loss 0.7074\n",
      "Epoch 9 Batch 200 Loss 0.6748\n",
      "Epoch 9 Batch 300 Loss 0.6652\n",
      "Epoch 9 Batch 400 Loss 0.7192\n",
      "Epoch 9 Batch 500 Loss 0.6189\n",
      "Epoch 9 Loss 0.659066\n",
      "Time taken for 1 epoch 93.52 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6217\n",
      "Epoch 10 Batch 100 Loss 0.6733\n",
      "Epoch 10 Batch 200 Loss 0.6091\n",
      "Epoch 10 Batch 300 Loss 0.6659\n",
      "Epoch 10 Batch 400 Loss 0.6890\n",
      "Epoch 10 Batch 500 Loss 0.6230\n",
      "Epoch 10 Loss 0.624472\n",
      "Time taken for 1 epoch 93.31 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.5705\n",
      "Epoch 11 Batch 100 Loss 0.5973\n",
      "Epoch 11 Batch 200 Loss 0.5299\n",
      "Epoch 11 Batch 300 Loss 0.5891\n",
      "Epoch 11 Batch 400 Loss 0.5287\n",
      "Epoch 11 Batch 500 Loss 0.5766\n",
      "Epoch 11 Loss 0.592941\n",
      "Time taken for 1 epoch 93.55 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.5969\n",
      "Epoch 12 Batch 100 Loss 0.4975\n",
      "Epoch 12 Batch 200 Loss 0.6639\n",
      "Epoch 12 Batch 300 Loss 0.6058\n",
      "Epoch 12 Batch 400 Loss 0.5996\n",
      "Epoch 12 Batch 500 Loss 0.5955\n",
      "Epoch 12 Loss 0.564574\n",
      "Time taken for 1 epoch 93.58 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.5283\n",
      "Epoch 13 Batch 100 Loss 0.5628\n",
      "Epoch 13 Batch 200 Loss 0.5022\n",
      "Epoch 13 Batch 300 Loss 0.5498\n",
      "Epoch 13 Batch 400 Loss 0.5433\n",
      "Epoch 13 Batch 500 Loss 0.4873\n",
      "Epoch 13 Loss 0.538235\n",
      "Time taken for 1 epoch 93.50 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.5703\n",
      "Epoch 14 Batch 100 Loss 0.5523\n",
      "Epoch 14 Batch 200 Loss 0.5418\n",
      "Epoch 14 Batch 300 Loss 0.4552\n",
      "Epoch 14 Batch 400 Loss 0.5087\n",
      "Epoch 14 Batch 500 Loss 0.5295\n",
      "Epoch 14 Loss 0.513595\n",
      "Time taken for 1 epoch 93.49 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.4991\n",
      "Epoch 15 Batch 100 Loss 0.4441\n",
      "Epoch 15 Batch 200 Loss 0.5442\n",
      "Epoch 15 Batch 300 Loss 0.5408\n",
      "Epoch 15 Batch 400 Loss 0.4855\n",
      "Epoch 15 Batch 500 Loss 0.4754\n",
      "Epoch 15 Loss 0.489472\n",
      "Time taken for 1 epoch 93.62 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.5014\n",
      "Epoch 16 Batch 100 Loss 0.4698\n",
      "Epoch 16 Batch 200 Loss 0.4457\n",
      "Epoch 16 Batch 300 Loss 0.4841\n",
      "Epoch 16 Batch 400 Loss 0.4541\n",
      "Epoch 16 Batch 500 Loss 0.4709\n",
      "Epoch 16 Loss 0.468393\n",
      "Time taken for 1 epoch 93.47 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.4615\n",
      "Epoch 17 Batch 100 Loss 0.4293\n",
      "Epoch 17 Batch 200 Loss 0.4299\n",
      "Epoch 17 Batch 300 Loss 0.4815\n",
      "Epoch 17 Batch 400 Loss 0.4567\n",
      "Epoch 17 Batch 500 Loss 0.4532\n",
      "Epoch 17 Loss 0.449291\n",
      "Time taken for 1 epoch 93.63 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.4966\n",
      "Epoch 18 Batch 100 Loss 0.4340\n",
      "Epoch 18 Batch 200 Loss 0.4305\n",
      "Epoch 18 Batch 300 Loss 0.4016\n",
      "Epoch 18 Batch 400 Loss 0.4265\n",
      "Epoch 18 Batch 500 Loss 0.4381\n",
      "Epoch 18 Loss 0.430123\n",
      "Time taken for 1 epoch 93.58 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5041\n",
      "Epoch 19 Batch 100 Loss 0.3937\n",
      "Epoch 19 Batch 200 Loss 0.4608\n",
      "Epoch 19 Batch 300 Loss 0.4255\n",
      "Epoch 19 Batch 400 Loss 0.4426\n",
      "Epoch 19 Batch 500 Loss 0.3860\n",
      "Epoch 19 Loss 0.413217\n",
      "Time taken for 1 epoch 93.55 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.4819\n",
      "Epoch 20 Batch 100 Loss 0.3933\n",
      "Epoch 20 Batch 200 Loss 0.3787\n",
      "Epoch 20 Batch 300 Loss 0.3422\n"
     ]
    }
   ],
   "source": [
    "# training network\n",
    "start_epoch = 0\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "\tstart = time.time()\n",
    "\ttotal_loss = 0\n",
    "\tfor (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "\t\tbatch_loss, t_loss = train_step(img_tensor, target, wordtoix)\n",
    "\t\ttotal_loss += t_loss\n",
    "\t\tif batch % 100 == 0:\n",
    "\t\t\taverage_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "\t\t\tprint(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "\t# storing the epoch end loss value to plot later\n",
    "\tloss_values.append(total_loss / num_steps)\n",
    "\tprint(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "\tprint(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "ckpt_manager.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
