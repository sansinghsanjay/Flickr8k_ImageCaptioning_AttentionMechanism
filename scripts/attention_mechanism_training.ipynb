{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1619816813399,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "CgKbbYxJ5t8q",
    "outputId": "396983bf-e8f8-481f-ec89-4faf2b16c6bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nSanjay Singh\\nsan.singhsanjay@gmail.com\\nApril-2021\\nImplementation of Attention Mechanism for Image Captioning - Most part taken from Google Tutorials\\nImplementation of:\\nhttps://github.com/subhamio/image-captioning-using-attention-mechanism-local-attention-and-global-attention-/blob/master/image_captioning_using_attention_mechanism.ipynb\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sanjay Singh\n",
    "san.singhsanjay@gmail.com\n",
    "April-2021\n",
    "Implementation of Attention Mechanism for Image Captioning - Most part taken from Google Tutorials\n",
    "Implementation of:\n",
    "https://github.com/subhamio/image-captioning-using-attention-mechanism-local-attention-and-global-attention-/blob/master/image_captioning_using_attention_mechanism.ipynb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1864,
     "status": "ok",
     "timestamp": 1619816814051,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "MMwgVmai5_t4",
    "outputId": "18645127-7395-45f8-9e47-f8be3094556e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTKEHdjOASw6"
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "import sys, time, os, warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.utils import shuffle\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvEwL-9BHA0W"
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 131\n",
    "BUFFER_SIZE = 1000\n",
    "UNITS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmqIz8RCH15E"
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "class VGG16_Encoder(tf.keras.Model):\n",
    "\t# This encoder passes the features through a Fully connected layer\n",
    "\tdef __init__(self, EMBEDDING_DIM):\n",
    "\t\tsuper(VGG16_Encoder, self).__init__()\n",
    "\t\tself.fc = tf.keras.layers.Dense(EMBEDDING_DIM)\n",
    "\t\tself.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.fc(x)\n",
    "\t\tx = tf.nn.relu(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SAhYeGOH7vv"
   },
   "outputs": [],
   "source": [
    "class Rnn_Local_Decoder(tf.keras.Model):\n",
    "\tdef __init__(self, embedding_dim, UNITS, vocab_size):\n",
    "\t\tsuper(Rnn_Local_Decoder, self).__init__()\n",
    "\t\tself.units = UNITS\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.gru = tf.compat.v1.keras.layers.CuDNNGRU(self.units, return_sequences=True, return_state=True,                              recurrent_initializer='glorot_uniform')\n",
    "\t\tself.fc1 = tf.keras.layers.Dense(self.units)\n",
    "\t\tself.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "\t\tself.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\t\tself.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\t\t# Implementing Attention Mechanism\n",
    "\t\tself.Uattn = tf.keras.layers.Dense(UNITS)\n",
    "\t\tself.Wattn = tf.keras.layers.Dense(UNITS)\n",
    "\t\tself.Vattn = tf.keras.layers.Dense(1)\n",
    "\n",
    "\tdef call(self, x, features, hidden):\n",
    "\t\t# features shape ==> (64,49,256) ==> Output from ENCODER\n",
    "\t\t# hidden shape == (batch_size, hidden_size) ==>(64,512)\n",
    "\t\t# hidden_with_time_axis shape == (batch_size, 1, hidden_size) ==> (64,1,512)\n",
    "\t\thidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\t\t# score shape == (64, 49, 1)\n",
    "\t\t# Attention Function\n",
    "\t\t'''e(ij) = f(s(t-1),h(j))'''\n",
    "\t\t''' e(ij) = Vattn(T)*tanh(Uattn * h(j) + Wattn * s(t))'''\n",
    "\t\tscore = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n",
    "\t\t# self.Uattn(features) : (64,49,512)\n",
    "\t\t# self.Wattn(hidden_with_time_axis) : (64,1,512)\n",
    "\t\t# tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)) : (64,49,512)\n",
    "\t\t# self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis))) : (64,49,1) ==> score\n",
    "\t\t# you get 1 at the last axis because you are applying score to self.Vattn\n",
    "\t\t# Then find Probability using Softmax\n",
    "\t\t'''attention_weights(alpha(ij)) = softmax(e(ij))'''\n",
    "\t\tattention_weights = tf.nn.softmax(score, axis=1)\n",
    "\t\t# attention_weights shape == (64, 49, 1)\n",
    "\t\t# Give weights to the different pixels in the image\n",
    "\t\t''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n",
    "\t\tcontext_vector = attention_weights * features\n",
    "\t\tcontext_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\t\t# Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n",
    "\t\t# context_vector shape after sum == (64, 256)\n",
    "\t\t# x shape after passing through embedding == (64, 1, 256)\n",
    "\t\tx = self.embedding(x)\n",
    "\t\t# x shape after concatenation == (64, 1,  512)\n",
    "\t\tx = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\t\t# passing the concatenated vector to the GRU\n",
    "\t\toutput, state = self.gru(x)\n",
    "\t\t# shape == (batch_size, max_length, hidden_size)\n",
    "\t\tx = self.fc1(output)\n",
    "\t\t# x shape == (batch_size * max_length, hidden_size)\n",
    "\t\tx = tf.reshape(x, (-1, x.shape[2]))\n",
    "\t\t# Adding Dropout and BatchNorm Layers\n",
    "\t\tx= self.dropout(x)\n",
    "\t\tx= self.batchnormalization(x)\n",
    "\t\t# output shape == (64 * 512)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\t# shape : (64 * 8329(vocab))\n",
    "\t\treturn x, state, attention_weights\n",
    "\n",
    "\tdef reset_state(self, batch_size):\n",
    "\t\treturn tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B9aUr_2IAYV"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "\tmask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "\tloss_ = loss_object(real, pred)\n",
    "\tmask = tf.cast(mask, dtype=loss_.dtype)\n",
    "\tloss_ *= mask\n",
    "\treturn tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY8XNty0IETK"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "\tloss = 0\n",
    "\t# initializing the hidden state for each batch\n",
    "\t# because the captions are not related from image to image\n",
    "\thidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\tdec_input = tf.expand_dims([wordtoix['startseq']] * BATCH_SIZE, 1)\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tfeatures = encoder(img_tensor)\n",
    "\t\tfor i in range(1, target.shape[1]):\n",
    "\t\t\t# passing the features through the decoder\n",
    "\t\t\tpredictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\t\t\tloss += loss_function(target[:, i], predictions)\n",
    "\t\t\t# using teacher forcing\n",
    "\t\t\tdec_input = tf.expand_dims(target[:, i], 1)\n",
    "\ttotal_loss = (loss / int(target.shape[1]))\n",
    "\ttrainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\tgradients = tape.gradient(loss, trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\treturn loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13LIAyZuIIt1"
   },
   "outputs": [],
   "source": [
    "# function to read, resize and preprocess an image\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img = preprocess_input(img)\n",
    "    return img, image_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEgBBBu7INJM"
   },
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "\timg_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "\treturn img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55Tl9qX9IQb4"
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "images_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/train_npy_files/\"\n",
    "img_captions_csv_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/processed_data_AttentionMech/train_image_caption_processed.csv\"\n",
    "vocabulary_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/processed_data_AttentionMech/vocabulary.txt\"\n",
    "max_caption_len_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/processed_data_AttentionMech/max_caption_length.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmTtV8YMIUQb"
   },
   "outputs": [],
   "source": [
    "# reading dataset\n",
    "data = pd.read_csv(img_captions_csv_path)\n",
    "img_name = list(data['image'])\n",
    "img_caption = list(data['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBsiooRXIZNo"
   },
   "outputs": [],
   "source": [
    "# appending image names to their paths\n",
    "for i in range(len(img_name)):\n",
    "\timg_name[i] = images_path + img_name[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOjBkjNMIc_q"
   },
   "outputs": [],
   "source": [
    "# reading vocabulary file\n",
    "vocabulary = list()\n",
    "f_ptr = open(vocabulary_path, \"r\")\n",
    "lines = f_ptr.readlines()\n",
    "for line in lines:\n",
    "\tvocabulary.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3927,
     "status": "ok",
     "timestamp": 1619816816317,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "0q2-rZk_IuMg",
    "outputId": "7e1d0430-7566-4444-f27e-713d214f296a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  1657\n"
     ]
    }
   ],
   "source": [
    "# finding size of vocabulary\n",
    "vocab_size = len(vocabulary) + 1 # added 1 for padded zero\n",
    "print(\"Vocabulary Size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-krs1w8Ixx8"
   },
   "outputs": [],
   "source": [
    "# making word-to-index and index-to-word dictionary\n",
    "wordtoix = dict()\n",
    "ixtoword = dict()\n",
    "for i in range(len(vocabulary)):\n",
    "\twordtoix[vocabulary[i]] = (i + 1)\n",
    "\tixtoword[(i + 1)] = vocabulary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3889,
     "status": "ok",
     "timestamp": 1619816816321,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "9sQsht1PI2Rf",
    "outputId": "d10630c3-8d90-4ee9-b8ad-14915fbe29f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Caption Length:  31\n"
     ]
    }
   ],
   "source": [
    "# finding max caption length\n",
    "f_ptr = open(max_caption_len_path, 'r')\n",
    "data = f_ptr.read()\n",
    "f_ptr.close()\n",
    "max_caption_len = int(data.split(\":\")[1].strip())\n",
    "print(\"Max Caption Length: \", max_caption_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3867,
     "status": "ok",
     "timestamp": 1619816816323,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "xsDqz0oaI529",
    "outputId": "394b5270-db6a-49c3-8add-c0451a7f008c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all_img_names:  29999\n",
      "Length of all_captions:  29999\n"
     ]
    }
   ],
   "source": [
    "# converting captions to their indices\n",
    "all_img_names = list()\n",
    "all_captions = list()\n",
    "img_caption_ix = list()\n",
    "for i in range(len(img_caption)):\n",
    "\tcaptions = img_caption[i].split(\"#\")\n",
    "\tfor caption in captions:\n",
    "\t\tall_img_names.append(img_name[i])\n",
    "\t\tall_captions.append(caption)\n",
    "\t\twords = caption.split(\" \")\n",
    "\t\ttemp_list = list()\n",
    "\t\tfor word in words:\n",
    "\t\t\tif(word in wordtoix):\n",
    "\t\t\t\ttemp_list.append(wordtoix[word])\n",
    "\t\timg_caption_ix.append(temp_list)\n",
    "# printing shape of all_img_names and all_captions\n",
    "print(\"Length of all_img_names: \", len(all_img_names))\n",
    "print(\"Length of all_captions: \", len(all_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3844,
     "status": "ok",
     "timestamp": 1619816816326,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "2P2tGjIUI9VL",
    "outputId": "bb99318a-10ef-40fb-88b6-496780ab86ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of img_caption_padded:  (29999, 31)\n"
     ]
    }
   ],
   "source": [
    "# padding zeros to each caption to make it equal to max_caption_len\n",
    "img_caption_padded = tf.keras.preprocessing.sequence.pad_sequences(img_caption_ix, max_caption_len, padding='post')\n",
    "print(\"Shape of img_caption_padded: \", img_caption_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "350YZjqJJEZW"
   },
   "outputs": [],
   "source": [
    "# loading vgg-16 model to extract bottleneck features\n",
    "image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 5253,
     "status": "ok",
     "timestamp": 1619816817780,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "n91KTNQ9JHrE",
    "outputId": "f9b4157c-746b-4d86-f569-553b0c107c07"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# to map data to loading function - to create npy files\\nencode_train = sorted(set(img_name))\\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\\n\\n# saving npy files\\nfor img, path in tqdm(image_dataset):\\n\\tbatch_features = image_features_extract_model(img) # batch_features.shape: [BATCH_SIZE, 7, 7, 512]\\n\\tbatch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3])) # batch_features.shape: [BATCH_SIZE, 49, 512]\\n\\tfor bf, p in zip(batch_features, path):\\n\\t\\tpath_of_feature = p.numpy().decode(\"utf-8\")\\n\\t\\tnp.save(path_of_feature, bf.numpy())\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# to map data to loading function - to create npy files\n",
    "encode_train = sorted(set(img_name))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "# saving npy files\n",
    "for img, path in tqdm(image_dataset):\n",
    "\tbatch_features = image_features_extract_model(img) # batch_features.shape: [BATCH_SIZE, 7, 7, 512]\n",
    "\tbatch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3])) # batch_features.shape: [BATCH_SIZE, 49, 512]\n",
    "\tfor bf, p in zip(batch_features, path):\n",
    "\t\tpath_of_feature = p.numpy().decode(\"utf-8\")\n",
    "\t\tnp.save(path_of_feature, bf.numpy())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJfbd79iJLoo"
   },
   "outputs": [],
   "source": [
    "# defining optimization and loss-object\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t95e67JJoXX"
   },
   "outputs": [],
   "source": [
    "# making image and caption map - for training\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_img_names, img_caption_padded))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axHZcVYDJq3G"
   },
   "outputs": [],
   "source": [
    "# defining encoder and decoder\n",
    "encoder = VGG16_Encoder(EMBEDDING_DIM)\n",
    "decoder = Rnn_Local_Decoder(EMBEDDING_DIM, UNITS, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5207,
     "status": "ok",
     "timestamp": 1619816817794,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "j0pz6tAbJumO",
    "outputId": "8148d30c-76ab-41d6-8778-e033b51699da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps:  229\n"
     ]
    }
   ],
   "source": [
    "# defining num_steps \n",
    "num_steps = len(all_img_names) // BATCH_SIZE\n",
    "print(\"num_steps: \", num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858550,
     "status": "ok",
     "timestamp": 1619817671166,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "4dl4ozmGJxGH",
    "outputId": "a1558263-ae42-4ead-b855-770754d51eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch-1 and batch-1: 22.46990442276001 sec, Loss: 2.2415020850396927\n",
      "\n",
      "Time taken for epoch-1 and batch-101: 40.396193981170654 sec, Loss: 1.4331924684586064\n",
      "\n",
      "Time taken for epoch-1 and batch-201: 58.300397872924805 sec, Loss: 1.3033741366478704\n",
      "\n",
      "Epoch 1 Loss 1.428742\n",
      "Time taken for 1 epoch 62.9291877746582 sec\n",
      "\n",
      "Time taken for epoch-2 and batch-1: 1.2008726596832275 sec, Loss: 1.209778078140751\n",
      "\n",
      "Time taken for epoch-2 and batch-101: 19.19206929206848 sec, Loss: 1.134524929908014\n",
      "\n",
      "Time taken for epoch-2 and batch-201: 37.11392569541931 sec, Loss: 1.1454983372842111\n",
      "\n",
      "Epoch 2 Loss 1.109071\n",
      "Time taken for 1 epoch 41.70030665397644 sec\n",
      "\n",
      "Time taken for epoch-3 and batch-1: 1.2028591632843018 sec, Loss: 1.1055418445217995\n",
      "\n",
      "Time taken for epoch-3 and batch-101: 18.98026704788208 sec, Loss: 1.0486766446021296\n",
      "\n",
      "Time taken for epoch-3 and batch-201: 36.8588342666626 sec, Loss: 1.0149089444068171\n",
      "\n",
      "Epoch 3 Loss 1.005771\n",
      "Time taken for 1 epoch 41.48889517784119 sec\n",
      "\n",
      "Time taken for epoch-4 and batch-1: 1.1885771751403809 sec, Loss: 0.9849222244754914\n",
      "\n",
      "Time taken for epoch-4 and batch-101: 19.21122908592224 sec, Loss: 0.913664787046371\n",
      "\n",
      "Time taken for epoch-4 and batch-201: 36.992432594299316 sec, Loss: 0.8928578284478956\n",
      "\n",
      "Epoch 4 Loss 0.936169\n",
      "Time taken for 1 epoch 41.58589839935303 sec\n",
      "\n",
      "Time taken for epoch-5 and batch-1: 1.22035551071167 sec, Loss: 0.9294919044740738\n",
      "\n",
      "Time taken for epoch-5 and batch-101: 19.219409704208374 sec, Loss: 0.8642624885805191\n",
      "\n",
      "Time taken for epoch-5 and batch-201: 37.004037380218506 sec, Loss: 0.8710164716166835\n",
      "\n",
      "Epoch 5 Loss 0.881170\n",
      "Time taken for 1 epoch 41.66939306259155 sec\n",
      "\n",
      "Time taken for epoch-6 and batch-1: 1.2353579998016357 sec, Loss: 0.8416786193847656\n",
      "\n",
      "Time taken for epoch-6 and batch-101: 19.283052682876587 sec, Loss: 0.8753468298142956\n",
      "\n",
      "Time taken for epoch-6 and batch-201: 37.111560106277466 sec, Loss: 0.839171624952747\n",
      "\n",
      "Epoch 6 Loss 0.834397\n",
      "Time taken for 1 epoch 41.684582471847534 sec\n",
      "\n",
      "Time taken for epoch-7 and batch-1: 1.2159397602081299 sec, Loss: 0.8200633141302294\n",
      "\n",
      "Time taken for epoch-7 and batch-101: 18.976569414138794 sec, Loss: 0.8521681139546056\n",
      "\n",
      "Time taken for epoch-7 and batch-201: 36.861491441726685 sec, Loss: 0.7849399197486139\n",
      "\n",
      "Epoch 7 Loss 0.792829\n",
      "Time taken for 1 epoch 41.495150566101074 sec\n",
      "\n",
      "Time taken for epoch-8 and batch-1: 1.1879644393920898 sec, Loss: 0.7499576691658266\n",
      "\n",
      "Time taken for epoch-8 and batch-101: 19.095010995864868 sec, Loss: 0.7400607447470388\n",
      "\n",
      "Time taken for epoch-8 and batch-201: 36.98033094406128 sec, Loss: 0.7303379428002142\n",
      "\n",
      "Epoch 8 Loss 0.754435\n",
      "Time taken for 1 epoch 41.57034420967102 sec\n",
      "\n",
      "Time taken for epoch-9 and batch-1: 1.1893939971923828 sec, Loss: 0.7370818353468372\n",
      "\n",
      "Time taken for epoch-9 and batch-101: 19.091013431549072 sec, Loss: 0.7626088050103956\n",
      "\n",
      "Time taken for epoch-9 and batch-201: 36.79720687866211 sec, Loss: 0.7428870662566154\n",
      "\n",
      "Epoch 9 Loss 0.720703\n",
      "Time taken for 1 epoch 41.420008182525635 sec\n",
      "\n",
      "Time taken for epoch-10 and batch-1: 1.2030954360961914 sec, Loss: 0.7501719074864541\n",
      "\n",
      "Time taken for epoch-10 and batch-101: 18.965484857559204 sec, Loss: 0.7464156612273185\n",
      "\n",
      "Time taken for epoch-10 and batch-201: 36.69989800453186 sec, Loss: 0.6601229021626134\n",
      "\n",
      "Epoch 10 Loss 0.688697\n",
      "Time taken for 1 epoch 41.316415786743164 sec\n",
      "\n",
      "Time taken for epoch-11 and batch-1: 1.2224342823028564 sec, Loss: 0.6936298493416079\n",
      "\n",
      "Time taken for epoch-11 and batch-101: 19.068481922149658 sec, Loss: 0.6733815593104209\n",
      "\n",
      "Time taken for epoch-11 and batch-201: 37.037312269210815 sec, Loss: 0.6929849193942162\n",
      "\n",
      "Epoch 11 Loss 0.660371\n",
      "Time taken for 1 epoch 41.69329118728638 sec\n",
      "\n",
      "Time taken for epoch-12 and batch-1: 1.2056431770324707 sec, Loss: 0.6279447001795615\n",
      "\n",
      "Time taken for epoch-12 and batch-101: 19.066656589508057 sec, Loss: 0.6396485605547505\n",
      "\n",
      "Time taken for epoch-12 and batch-201: 36.81794714927673 sec, Loss: 0.6322265748054751\n",
      "\n",
      "Epoch 12 Loss 0.633810\n",
      "Time taken for 1 epoch 41.40742015838623 sec\n",
      "\n",
      "Time taken for epoch-13 and batch-1: 1.2127327919006348 sec, Loss: 0.6408794772240424\n",
      "\n",
      "Time taken for epoch-13 and batch-101: 19.086496114730835 sec, Loss: 0.6352309565390309\n",
      "\n",
      "Time taken for epoch-13 and batch-201: 36.945308208465576 sec, Loss: 0.6107427535518524\n",
      "\n",
      "Epoch 13 Loss 0.607607\n",
      "Time taken for 1 epoch 41.580204010009766 sec\n",
      "\n",
      "Time taken for epoch-14 and batch-1: 1.1901218891143799 sec, Loss: 0.5681312314925655\n",
      "\n",
      "Time taken for epoch-14 and batch-101: 19.030577898025513 sec, Loss: 0.6180550975184287\n",
      "\n",
      "Time taken for epoch-14 and batch-201: 37.92170810699463 sec, Loss: 0.584274415046938\n",
      "\n",
      "Epoch 14 Loss 0.583265\n",
      "Time taken for 1 epoch 42.607117652893066 sec\n",
      "\n",
      "Time taken for epoch-15 and batch-1: 1.2114169597625732 sec, Loss: 0.6003933568154612\n",
      "\n",
      "Time taken for epoch-15 and batch-101: 19.036428213119507 sec, Loss: 0.5537569599766885\n",
      "\n",
      "Time taken for epoch-15 and batch-201: 36.83427119255066 sec, Loss: 0.5748652796591481\n",
      "\n",
      "Epoch 15 Loss 0.560865\n",
      "Time taken for 1 epoch 41.54032611846924 sec\n",
      "\n",
      "Time taken for epoch-16 and batch-1: 1.191676378250122 sec, Loss: 0.5652611024918095\n",
      "\n",
      "Time taken for epoch-16 and batch-101: 19.157564163208008 sec, Loss: 0.589234875094506\n",
      "\n",
      "Time taken for epoch-16 and batch-201: 37.000195026397705 sec, Loss: 0.5286467152257119\n",
      "\n",
      "Epoch 16 Loss 0.540590\n",
      "Time taken for 1 epoch 41.615052938461304 sec\n",
      "\n",
      "Time taken for epoch-17 and batch-1: 1.2480602264404297 sec, Loss: 0.5391997675741872\n",
      "\n",
      "Time taken for epoch-17 and batch-101: 19.01720118522644 sec, Loss: 0.5121207083425214\n",
      "\n",
      "Time taken for epoch-17 and batch-201: 36.81451439857483 sec, Loss: 0.5396963550198463\n",
      "\n",
      "Epoch 17 Loss 0.520692\n",
      "Time taken for 1 epoch 41.460567474365234 sec\n",
      "\n",
      "Time taken for epoch-18 and batch-1: 1.2604763507843018 sec, Loss: 0.5396990006969821\n",
      "\n",
      "Time taken for epoch-18 and batch-101: 19.165968656539917 sec, Loss: 0.5397945527107485\n",
      "\n",
      "Time taken for epoch-18 and batch-201: 37.00944495201111 sec, Loss: 0.5194556943831905\n",
      "\n",
      "Epoch 18 Loss 0.501490\n",
      "Time taken for 1 epoch 41.59951043128967 sec\n",
      "\n",
      "Time taken for epoch-19 and batch-1: 1.233036994934082 sec, Loss: 0.4999419489214497\n",
      "\n",
      "Time taken for epoch-19 and batch-101: 19.02323842048645 sec, Loss: 0.48070421526508944\n",
      "\n",
      "Time taken for epoch-19 and batch-201: 36.93465209007263 sec, Loss: 0.4804319566295993\n",
      "\n",
      "Epoch 19 Loss 0.483977\n",
      "Time taken for 1 epoch 41.56733226776123 sec\n",
      "\n",
      "Time taken for epoch-20 and batch-1: 1.1997451782226562 sec, Loss: 0.47721090624409335\n",
      "\n",
      "Time taken for epoch-20 and batch-101: 19.024826288223267 sec, Loss: 0.5036955495034495\n",
      "\n",
      "Time taken for epoch-20 and batch-201: 36.76341438293457 sec, Loss: 0.4642217697635774\n",
      "\n",
      "Epoch 20 Loss 0.468315\n",
      "Time taken for 1 epoch 41.48344898223877 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training related parameters and training\n",
    "loss_plot = []\n",
    "start_epoch = 0\n",
    "EPOCHS = 20\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        #target = tf.reshape(target, (1, target.shape[0]))\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Time taken for epoch-{} and batch-{}: {} sec, Loss: {}\\n\".format(epoch + 1, batch + 1, time.time() - start, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1189,
     "status": "ok",
     "timestamp": 1619818480798,
     "user": {
      "displayName": "Gpu User",
      "photoUrl": "",
      "userId": "05020043369902705889"
     },
     "user_tz": -330
    },
    "id": "AKuLFXhhtG1X",
    "outputId": "6e4aba22-4c9f-44c3-9dcf-f05178475d81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/output/attention_mech_models/ckpt-1'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"/content/gdrive/MyDrive/Flickr8k_ImageCaptioning/output/attention_mech_models/\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,decoder=decoder,optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "ckpt_manager.save()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMs3Gu9KcV/7q6uGtKDX8vh",
   "collapsed_sections": [],
   "name": "script_attention_mechanism.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
